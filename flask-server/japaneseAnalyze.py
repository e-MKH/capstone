import os
import re
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from google.cloud import language_v1
from fetchJapaneseNews import fetch_news  # âœ… ì™¸ë¶€ í•¨ìˆ˜ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv(dotenv_path=".env.japanese")
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

app = Flask(__name__)
client = language_v1.LanguageServiceClient()

# âœ… ì¼ë³¸ì–´ ë° ì˜ì–´ â†’ GNews ì¹´í…Œê³ ë¦¬ í‚¤ ë§¤í•‘
category_mapping = {
    # ì¼ë³¸ì–´ í‚¤
    "æ”¿æ²»": "politics",
    "çµŒæ¸ˆ": "business",
    "ç§‘å­¦": "science",
    "æŠ€è¡“": "technology",
    "ã‚¹ãƒãƒ¼ãƒ„": "sports",
    "å¥åº·": "health",
    "ã‚¨ãƒ³ã‚¿ãƒ¡": "entertainment",
    "å›½éš›": "world",

    # ì˜ì–´ í‚¤ (ì§ì ‘ ìš”ì²­ ë°©ì§€ìš©)
    "politics": "politics",
    "business": "business",
    "science": "science",
    "technology": "technology",
    "sports": "sports",
    "health": "health",
    "entertainment": "entertainment",
    "world": "world"
}

# âœ… ë‚œì´ë„ ë¶„ë¥˜ ê¸°ì¤€
def classify_level(score):
    if score >= 9.0:
        return "ì´ˆê¸‰"
    elif score >= 8.5:
        return "ì¤‘ê¸‰"
    else:
        return "ê³ ê¸‰"

# âœ… í˜•íƒœì†Œ ë¶„ì„ + ë‚œì´ë„ ê³„ì‚°
def analyze_text(text):
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)
    total_chars = sum(len(s) for s in sentences)
    total_sentences = len(sentences) or 1
    avg_sentence_length = total_chars / total_sentences

    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT, language="ja")
    response = client.analyze_syntax(request={"document": document})

    counts = {
        "words": 0,
        "kanji_words": 0,
        "wago_words": 0,
        "verbs": 0,
        "aux_verbs": 0
    }

    for token in response.tokens:
        surface = token.text.content
        tag = language_v1.PartOfSpeech.Tag(token.part_of_speech.tag).name
        counts["words"] += 1

        if re.search(r"[ä¸€-é¾¥]", surface):
            counts["kanji_words"] += 1
        elif re.fullmatch(r"[ã-ã‚“ãƒ¼]+", surface):
            counts["wago_words"] += 1

        if tag == "VERB":
            counts["verbs"] += 1
        elif tag == "AUXILIARY":
            counts["aux_verbs"] += 1

    total_words = counts["words"] or 1
    p_kanji = counts["kanji_words"] / total_words
    p_wago = counts["wago_words"] / total_words
    p_verb = counts["verbs"] / total_words
    p_aux = counts["aux_verbs"] / total_words

    score = (
        -0.056 * avg_sentence_length
        -0.126 * p_kanji
        -0.042 * p_wago
        -0.145 * p_verb
        -0.044 * p_aux
        + 11.724
    )

    return {
        "score": round(score, 3),
        "level": classify_level(score),
        "sentence_len": round(avg_sentence_length, 2),
        "kanji_ratio": round(p_kanji, 2),
        "wago_ratio": round(p_wago, 2),
        "verb_ratio": round(p_verb, 2),
        "particle_ratio": round(p_aux, 2),
        "sentences_analyzed": total_sentences
    }

# âœ… í†µí•©ëœ GET/POST ë¼ìš°íŠ¸
@app.route("/analyze-japanese-news", methods=["GET", "POST"])
def analyze_japanese_news():
    try:
        if request.method == "POST":
            data = request.get_json()
            text = data.get("text", "")
            if not text.strip():
                return jsonify({"error": "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}), 400

            print(f"ğŸ“© ë‹¨ì¼ ê¸°ì‚¬ ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ ë¨. ê¸¸ì´: {len(text)}ì")
            analysis = analyze_text(text)
            print(f"âœ… ë¶„ì„ ì™„ë£Œ â†’ ì ìˆ˜: {analysis['score']}, ë ˆë²¨: {analysis['level']}")
            return jsonify(analysis)

        else:  # GET ë°©ì‹: categoryë³„ ë‰´ìŠ¤ ë¶„ì„
            original = request.args.get("category", "æ”¿æ²»")
            category = category_mapping.get(original, "general")
            print(f"ğŸ” ì¹´í…Œê³ ë¦¬ ë¶„ì„ ìš”ì²­: {original} â†’ {category}")

            articles = fetch_news(category)
            print(f"ğŸ“° ê°€ì ¸ì˜¨ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

            results = []
            for article in articles:
                title = article.get("title", "")
                desc = article.get("description", "")
                combined_text = f"{title}\n{desc}".strip()

                if not combined_text:
                    print("âš ï¸ ë¹ˆ ê¸°ì‚¬ ìŠ¤í‚µë¨")
                    continue

                analysis = analyze_text(combined_text)
                print(f"âœ… ë¶„ì„ ì™„ë£Œ: {title[:30]}... â†’ ì ìˆ˜: {analysis['score']}, ë ˆë²¨: {analysis['level']}")

                results.append({
                    "original": title,
                    "url": article.get("link", ""),
                    "description": desc,
                    "publishedAt": article.get("publishedAt", ""),
                    "analysis": analysis
                })

            return jsonify({
                "keyword": original,
                "results": results
            })

    except Exception as e:
        print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return jsonify({"error": str(e)}), 500

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    app.run(debug=True, port=6100)
